### Переобучение и дисперсия

# Минимизация квадрата ошибки сама по себе ничего не гарантирует (просто один из способов)
# Цель состоит не в минимизации суммы квадратов, а в том чтобы делать "правильные" предсказания на новых данных
# Переобученные модели (сильно подстроенные под исходные данные - графически: не усредненная прямая, а сплайн и т.п.) очень чувствительны к выбросам,
# которые находятся далеко от остальных точек. В прогнозах будет очень высокая дисперсия (отклонение от среднего)
# Поэтому к моделям специально добавляется смещение. Это означает, что предпочтение отдается определенной схеме (например прямая линия или линия через (0,0)),
# а не графикам со сложной структурой, минимизирующей остатки
# Но если в модель добавить смещение, то есть риск недообучения
# Задача сводится к балансировке между минимизацией функции потерь (суммы квадратов остатков) и смещением
# Существует 2 популярных варианта регрессии:
# - Гребневая регрессия (ridge): добавляется смещение в виде штрафа, из-за этого хуже идет подгонка
# - Лассо-регрессия: удаление некоторых переменных ("неважных"), тем самым снижается размерность и вносится смещение

# Механически применить линейную регрессию к данным, сделать на основе полученной модели прогноз, и думать что все в порядке - НЕЛЬЗЯ
# Всегда нужно анализировать модель на предмет переобучения и недообучения - искать "золотую середину"
# Но ее можно и не найти и сделать тогда вывод, что регрессионная модель не подходит под конкретную задачу

import numpy as np
import pandas as pd
from joblib.parallel import method
from numpy.f2py.cb_rules import cb_map
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import matplotlib.pyplot as plt
from numpy.linalg import inv, qr
import random
import pandas
from sympy.abc import alpha

data = np.array(
    [
        [1, 5],
        [2, 7],
        [3, 7],
        [4, 10],
        [5, 11],
        [6, 14],
        [7, 17],
        [8, 19],
        [9, 22],
        [10, 28],
    ]
)
### Градиентный спуск
# Использованный в лекции2 градиентный спуск называется пакетным. Для работы используются ВСЕ доступные обучающие данные
# На практике используется Стохастический градиентный спуск (вероятностный) - на каждой итерации обучаемся только по одной выборке из данных

# x = data[:,0]
# y = data[:,1]
# n = len(x)
#
# w1 = 0.0
# w0 = 0.0
#
# L = 0.001 # Шаг, ~Скорость обучения
# iterations = 100_000
#
# # Размер выборки:
# sample_size = 1
#
# # Берем частные производные функции потерь и обновляем значение коэф-в w0 w1
# for i in range(iterations):
#     idx = np.random.choice(n, sample_size, replace=False) # replace - возможность повтора выбора одного и того же элемента
#     D_w0 = 2 * sum(-y[idx] + w0 + w1 * x[idx])
#     D_w1 = 2 * sum((x[idx] * (-y[idx] + w0 + w1 * x[idx])))
#     w1 -= L * D_w1
#     w0 -= L * D_w0
#
# print(w1, w0)

# Присутствует рандом - поэтому каждый раз значения получаются немного разными
# - сокращение числа вычислений
# - вносим смещение и боремся с переобучением (засчет случайности и использования не всей выборки)

## Если sample_size != 1 то получаем:
# Мини-пакетный градиентный спуск - на каждой итерации используется несколько выборок



### Как оценить насколько сильно промахиваются прогнозы при использовании линейной регрессии

# Для оценки степени взаимосвязи между двумя переменными использовали линейный коэф-т корреляции

data_df = pd.DataFrame(data)
# print(data_df.corr(method='pearson')) # корреляционная матрица
#
# data_df[1] = data_df[1].values[::-1] # столбик у в обратном порядке
# print(data_df.corr(method='pearson'))

# Коэффициент корреляции помогает понять, есть ли связь между двумя элементами (ближе к 1 или -1 связь прямая или обратная, ближе к 0 - связь слабеет)


## Обучающие и тестовые выборки - основной метод борьбы с переобучением
# Заключается в том, что набор данных делится на обучающую и тестовую выборку
# Во всех видах машинного обучения с учителем это встречается
# Обычная пропорция: 2/3 - на обучение, 1/3 - на тест (или 4/5 к 1/5, 9/10 к 1/10)

X = data_df.values[:,:-1]
Y = data_df.values[:, 1]
print(X)
print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)
print(X_train)
print(Y_train)
print(X_test)
print(Y_test)

model = LinearRegression()
model.fit(X_train, Y_train)

r = model.score(X_test, Y_test) # Коэф-т детерминации (в данном случае переменная r = r^2)
print(r)
# Чем ближе к 1 - тем лучше регрессия работает на тестовых данных, чем ближе к 0 - тем хуже

## Перекрестная валидация
# У нас набор данных поделился на 3 части (2 на обучение и 1 на тест)
# В данном случае для теста берут поочередно каждую из трех частей и трижды обучают

kfold = KFold(n_splits=3, random_state=1, shuffle=True) # 3-х кратная перекрестная валидация
model = LinearRegression()
results = cross_val_score(model, X, Y, cv=kfold)

print(results) # массив среднеквадратических ошибок для всех трех моделей
print(results.mean(), results.std()) # для всех моделей среднее значение и стандартное отклонение

# Эти метрики показывают насколько ЕДИНООБРАЗНО ведет себя модель на разных выборках

# Возможно использование поэлементной перекрестной валидации (тестовая выборка - 1 точка данных) - когда мало данных
# -> (n_splits=10 для нашего примера) - поэлементная валидация (тк 10 точек)
# Случайная валидация - позволяет перемешивать данные и разделять на 2 выборки многократно (все данные переиспользуются многократно) - данные сильно разбросаны

# Иногда часть данных резервируют под валидационную выборку - для сравнения работы различных моделей или конфигураций


## Многомерная линейная регрессия

data_df = pd.read_csv('multiple_independent_variable_linear.csv')
print(data_df.head())

X = data_df.values[:,:-1] # Все столбцы из значений кроме последнего
Y = data_df.values[:,-1] # Последний столбец из матрицы значений (без индексов засчет .values)

model = LinearRegression().fit(X, Y)

print(model.coef_, model.intercept_) # y = a0 + a1*x1 + a2*x2

x1 = X[:, 0]
x2 = X[:, 1]
y = Y
fig = plt.figure()
ax = plt.axes(projection='3d')
ax.scatter3D(x1, x2, y)

x1_ = np.linspace(min(x1), max(x1), 100)
x2_ = np.linspace(min(x2), max(x2), 100)
X1_, X2_ = np.meshgrid(x1_, x2_)

Y_ = model.intercept_ + model.coef_[0] * X1_ + model.coef_[1] * X2_

ax. plot_surface(X1_, X2_, Y_, cmap='Greys', alpha=0.2)

plt.show()